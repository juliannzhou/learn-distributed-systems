{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Systems\n",
    "\n",
    "## Lecture 1 Introduction\n",
    "Distributed Systems: a group of cooperating computers communicating with each other over network to get some tasks done.\n",
    "\n",
    "### Use cases\n",
    "- storage for big website\n",
    "- big data computation （e.g. MapReduce）\n",
    "\n",
    "### Why distributed systems?\n",
    "- parallelism\n",
    "- fault tolerance\n",
    "- physical reasons \n",
    "- security\n",
    "\n",
    "### Challenges\n",
    "- concurrency\n",
    "- partial failure\n",
    "- performance\n",
    "\n",
    "### Infrasctructure\n",
    "The goal is to build distributed applications that act as single systems and abstract away the distributedness.\n",
    "\n",
    "#### Types\n",
    "- storage\n",
    "- communication\n",
    "- computation\n",
    "\n",
    "### Topics\n",
    "- implementation \n",
    "  - RPC, threads, concurrency\n",
    "- performance\n",
    "  - scalability\n",
    "    - full scalability: 2 times the computers / resources gets 2 times the throughput\n",
    "    - rare to get full scalability due to system bottlenecks\n",
    "  - fault tolerance\n",
    "    - large-scale applications contain many components prone to failure -> \n",
    "    - the ability to perceive and mask a failure is required\n",
    "    - what it means to be fault tolerant?\n",
    "      - availability: the system can keep operating despite the failure of some of its component and provide undamaged service\n",
    "      - recoverability: the system can continue operating after failures without loss of correctness\n",
    "        - tools: \n",
    "          - non-volatile storage \n",
    "            - to store checkpoints or logs of the state of the system\n",
    "            - challenges: management of NV systems due to expensiveness to update -> \n",
    "            - need clever ways to avoid writes to NV storage\n",
    "          - replication\n",
    "            - challenges: syncing the replicas\n",
    "            - replicas should have different independent failure probability\n",
    "  - consistency\n",
    "    - operations\n",
    "      - `put(k,v)`\n",
    "      - `get(k) -> v`\n",
    "    - challenges: \n",
    "      - more than one copy of the data in distributed systems ->\n",
    "      - may end up with two versions of data if put operations fail for some copies of data\n",
    "    - types of consistency\n",
    "      - strong consistency: `get` operations return the most recent put\n",
    "      - weak consistency: `get` operations do not guarante the most recent put and may return old values\n",
    "        - why useful: avoid excessive communication costs \n",
    "          - why expensive: replicas are often far apart for independent failure probability\n",
    "\n",
    "### Case study: MapReduce\n",
    "  - context: Google wants create a framework that abstract away the low-level implementation details of distributed systems and allow non-specialists to write and run distributed computations\n",
    "  - abstract view of MapReduce\n",
    "    - input is split into a different files with `map` and `reduce` operations\n",
    "  ```\n",
    "    * Map: split the input into words and for every word it sees emits the word as the key and 1 as value, then group together all instances of the same keys\n",
    "      \n",
    "    Map(k, v)\n",
    "      split v into words\n",
    "        for each word w\n",
    "          emit(w, \"1\")\n",
    "\n",
    "    * Reduce: sum up the total number of instances for every key\n",
    "\n",
    "    Reduce(k, v)\n",
    "      emit(len(v))\n",
    "\n",
    "    * Example of a MapReduce job:\n",
    "      input 1 ('ab') -> `Map` -> [(a, 1), (b, 1)] \n",
    "      input 2 ('b') -> `Map` -> [(b, 1)]\n",
    "      input 3 ('ac') -> `Map` -> [(a, 1), (c, 1)]\n",
    "      \n",
    "      -> \n",
    "\n",
    "      [(a, 1), (a, 1)] -> `Reduce` -> (a, 2)\n",
    "      [(b, 1)] -> `Reduce` -> (b, 1)\n",
    "      [(c, 1)] -> `Reduce` -> (c, 1)\n",
    "  ```\n",
    "- Q: Where does the data emitted by `Map` and `Reduce` functions go?\n",
    "  - underneath the MapReduce system there is some big collection of servers (worker servers) and a single master server orchestrating the systems\n",
    "    - the master server farms out `Map` invitations for a set of input files to worker servers\n",
    "    - worker servers read the input files, then call the `Map` functions with the assigned input files\n",
    "    - then worker will implement `emit` to write data to files to local disks to perform `Reduce` functions\n",
    "    - `Reduce` emits write the output to a file in Google's cluster file service \n",
    "  - GFS (Google File Service): a cluster file system that runs on the exactly same set of worker servers for MapReduce\n",
    "    - automatically splits up big files evenly across worker servers in 64MB chunks\n",
    "    - can then launch map workers in parallel ->\n",
    "    - result: increased read throughput\n",
    "- Q: How does inputs get correctly passed to `Map` functions?\n",
    "  - MapReduce worker process fetch the input stored in a GFS server\n",
    "  - bottleneck: network throughput for the fetch operations\n",
    "    - MapReduce and GFS servers communicate through some root Ethernet switch with limited throughput\n",
    "    - each machine shares 50 mbit/sec -> small compared to disk and CPU\n",
    "  - solutions: run GFS and MapReduce on the same machines\n",
    "    - fetching: master servers finds the server that holds the input file and send the map function to the input file on the same machine for local reads\n",
    "    - storing: stores the output of the map on the same disk\n",
    "  - limitations:\n",
    "    - shuffle: grouping of values under the same key still requires network operations -> expansive part of the MapReduce\n",
    "    - sorting: outputs have the same size as the inputs -> large amount of data\n",
    "  - modern data centers introduce multiple network switches -> no longer need to run MapReduce on the same machines\n",
    "- Q: How to run another map-reduce operation on the output of a map-reduce?\n",
    "  - outputs of map-reduce are stored on the GFS servers ->\n",
    "  - another round of network operations needed for getting outputs of reduce to GFS"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
